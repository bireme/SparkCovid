[0m[[0m[31merror[0m] [0m[0morg.apache.spark.SparkException: Task not serializable[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.clean(SparkContext.scala:2526)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1000)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.foreach(RDD.scala:999)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:3340)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4154)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4152)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3340)[0m
[0m[[0m[31merror[0m] [0m[0m	at sct.SparkCovidWho$.main(SparkCovidWho.scala:81)[0m
[0m[[0m[31merror[0m] [0m[0m	at sct.SparkCovidWho.main(SparkCovidWho.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1980)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1919)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:366)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.io.NotSerializableException: org.apache.log4j.Logger[0m
[0m[[0m[31merror[0m] [0m[0mSerialization stack:[0m
[0m[[0m[31merror[0m] [0m[0m	- object not serializable (class: org.apache.log4j.Logger, value: org.apache.log4j.Logger@7b12ef9f)[0m
[0m[[0m[31merror[0m] [0m[0m	- element of array (index: 0)[0m
[0m[[0m[31merror[0m] [0m[0m	- array (class [Ljava.lang.Object;, size 1)[0m
[0m[[0m[31merror[0m] [0m[0m	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)[0m
[0m[[0m[31merror[0m] [0m[0m	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class sct.SparkCovidWho$, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic sct/SparkCovidWho$.$anonfun$main$1$adapted:(Lorg/apache/log4j/Logger;Lorg/apache/spark/sql/Row;)Ljava/lang/Object;, instantiatedMethodType=(Lorg/apache/spark/sql/Row;)Ljava/lang/Object;, numCaptured=1])[0m
[0m[[0m[31merror[0m] [0m[0m	- writeReplace data (class: java.lang.invoke.SerializedLambda)[0m
[0m[[0m[31merror[0m] [0m[0m	- object (class sct.SparkCovidWho$$$Lambda$7917/1492840118, sct.SparkCovidWho$$$Lambda$7917/1492840118@2b565ca7)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.clean(SparkContext.scala:2526)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1000)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.foreach(RDD.scala:999)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:3340)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4154)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4152)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3340)[0m
[0m[[0m[31merror[0m] [0m[0m	at sct.SparkCovidWho$.main(SparkCovidWho.scala:81)[0m
[0m[[0m[31merror[0m] [0m[0m	at sct.SparkCovidWho.main(SparkCovidWho.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1980)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1919)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:366)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.SparkException: Task not serializable[0m
